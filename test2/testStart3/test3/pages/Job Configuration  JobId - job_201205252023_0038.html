





  
<html>
<head>
<title>Job Configuration: JobId - job_201205252023_0038</title>
<link rel="stylesheet" type="text/css" href="/static/hadoop.css">
<link rel="icon" type="image/vnd.microsoft.icon" href="/static/images/favicon.ico" />
</head>
<body>
<h2>Job Configuration: JobId - job_201205252023_0038</h2><br>

<table border="1" align="center" class="datatable">
<thead>
<tr>
<th>name</th><th>value</th>
</tr>
</thead>
<tbody>
<tr>
<td width="35%"><b>hive.skewjoin.mapjoin.map.tasks</b></td><td width="65%">10000</td>
</tr>
<tr>
<td width="35%"><b>job.end.retry.interval</b></td><td width="65%">30000</td>
</tr>
<tr>
<td width="35%"><b>hive.stats.jdbc.timeout</b></td><td width="65%">30</td>
</tr>
<tr>
<td width="35%"><b>io.bytes.per.checksum</b></td><td width="65%">512</td>
</tr>
<tr>
<td width="35%"><b>hive.hwi.listen.port</b></td><td width="65%">9999</td>
</tr>
<tr>
<td width="35%"><b>hive.skewjoin.key</b></td><td width="65%">100000</td>
</tr>
<tr>
<td width="35%"><b>mapred.job.tracker.retiredjobs.cache.size</b></td><td width="65%">1000</td>
</tr>
<tr>
<td width="35%"><b>mapred.queue.default.acl-administer-jobs</b></td><td width="65%">*</td>
</tr>
<tr>
<td width="35%"><b>mapred.task.profile.reduces</b></td><td width="65%">0-2</td>
</tr>
<tr>
<td width="35%"><b>mapreduce.jobtracker.staging.root.dir</b></td><td width="65%">${hadoop.tmp.dir}/mapred/staging</td>
</tr>
<tr>
<td width="35%"><b>mapreduce.job.cache.files.visibilities</b></td><td width="65%">true</td>
</tr>
<tr>
<td width="35%"><b>mapred.job.reuse.jvm.num.tasks</b></td><td width="65%">1</td>
</tr>
<tr>
<td width="35%"><b>dfs.block.access.token.lifetime</b></td><td width="65%">600</td>
</tr>
<tr>
<td width="35%"><b>javax.jdo.option.ConnectionURL</b></td><td width="65%">jdbc:mysql://localhost/metastore</td>
</tr>
<tr>
<td width="35%"><b>mapred.reduce.tasks.speculative.execution</b></td><td width="65%">true</td>
</tr>
<tr>
<td width="35%"><b>hive.mapred.mode</b></td><td width="65%">nonstrict</td>
</tr>
<tr>
<td width="35%"><b>mapred.job.name</b></td><td width="65%">INSERT OVERWRITE TABLE rankings_uservisi...1(Stage-4)</td>
</tr>
<tr>
<td width="35%"><b>hive.metastore.ds.retry.attempts</b></td><td width="65%">1</td>
</tr>
<tr>
<td width="35%"><b>hadoop.http.authentication.kerberos.keytab</b></td><td width="65%">${user.home}/hadoop.keytab</td>
</tr>
<tr>
<td width="35%"><b>dfs.permissions.supergroup</b></td><td width="65%">supergroup</td>
</tr>
<tr>
<td width="35%"><b>io.seqfile.sorter.recordlimit</b></td><td width="65%">1000000</td>
</tr>
<tr>
<td width="35%"><b>hadoop.relaxed.worker.version.check</b></td><td width="65%">true</td>
</tr>
<tr>
<td width="35%"><b>mapred.task.tracker.http.address</b></td><td width="65%">0.0.0.0:50060</td>
</tr>
<tr>
<td width="35%"><b>datanucleus.autoStartMechanismMode</b></td><td width="65%">checked</td>
</tr>
<tr>
<td width="35%"><b>dfs.namenode.delegation.token.renew-interval</b></td><td width="65%">86400000</td>
</tr>
<tr>
<td width="35%"><b>hive.mapjoin.maxsize</b></td><td width="65%">100000</td>
</tr>
<tr>
<td width="35%"><b>hadoop.workaround.non.threadsafe.getpwuid</b></td><td width="65%">false</td>
</tr>
<tr>
<td width="35%"><b>datanucleus.identifierFactory</b></td><td width="65%">datanucleus</td>
</tr>
<tr>
<td width="35%"><b>fs.ramfs.impl</b></td><td width="65%">org.apache.hadoop.fs.InMemoryFileSystem</td>
</tr>
<tr>
<td width="35%"><b>mapred.system.dir</b></td><td width="65%">${hadoop.tmp.dir}/mapred/system</td>
</tr>
<tr>
<td width="35%"><b>fs.s3.awsSecretAccessKey</b></td><td width="65%">cEiOvcieTmWFBhU/TosMmn+6vNwiXn+8KbcO0D6G</td>
</tr>
<tr>
<td width="35%"><b>hadoop.permitted.revisions</b></td><td width="65%">
03b655719d13929bd68bb2c2f9cee615b389cea9,
217a3767c48ad11d4632e19a22897677268c40c4
  </td>
</tr>
<tr>
<td width="35%"><b>mapred.task.tracker.report.address</b></td><td width="65%">127.0.0.1:0</td>
</tr>
<tr>
<td width="35%"><b>hive.hwi.war.file</b></td><td width="65%">/usr/lib/hive/lib/hive-hwi-0.7.1-cdh3u1.war</td>
</tr>
<tr>
<td width="35%"><b>hive.mapjoin.followby.gby.localtask.max.memory.usage</b></td><td width="65%">0.55</td>
</tr>
<tr>
<td width="35%"><b>mapreduce.reduce.shuffle.connect.timeout</b></td><td width="65%">180000</td>
</tr>
<tr>
<td width="35%"><b>mapreduce.job.counters.max</b></td><td width="65%">120</td>
</tr>
<tr>
<td width="35%"><b>mapred.healthChecker.interval</b></td><td width="65%">60000</td>
</tr>
<tr>
<td width="35%"><b>hive.input.format</b></td><td width="65%">org.apache.hadoop.hive.ql.io.CombineHiveInputFormat</td>
</tr>
<tr>
<td width="35%"><b>mapreduce.job.complete.cancel.delegation.tokens</b></td><td width="65%">true</td>
</tr>
<tr>
<td width="35%"><b>fs.trash.interval</b></td><td width="65%">0</td>
</tr>
<tr>
<td width="35%"><b>hive.heartbeat.interval</b></td><td width="65%">1000</td>
</tr>
<tr>
<td width="35%"><b>hive.session.silent</b></td><td width="65%">false</td>
</tr>
<tr>
<td width="35%"><b>hive.metastore.server.min.threads</b></td><td width="65%">200</td>
</tr>
<tr>
<td width="35%"><b>mapred.skip.map.auto.incr.proc.count</b></td><td width="65%">true</td>
</tr>
<tr>
<td width="35%"><b>hadoop.http.authentication.kerberos.principal</b></td><td width="65%">HTTP/localhost@LOCALHOST</td>
</tr>
<tr>
<td width="35%"><b>mapred.min.split.size.per.rack</b></td><td width="65%">1</td>
</tr>
<tr>
<td width="35%"><b>hive.skewjoin.mapjoin.min.split</b></td><td width="65%">33554432</td>
</tr>
<tr>
<td width="35%"><b>mapred.child.tmp</b></td><td width="65%">./tmp</td>
</tr>
<tr>
<td width="35%"><b>mapred.tasktracker.taskmemorymanager.monitoring-interval</b></td><td width="65%">5000</td>
</tr>
<tr>
<td width="35%"><b>hive.script.serde</b></td><td width="65%">org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe</td>
</tr>
<tr>
<td width="35%"><b>fs.s3n.awsAccessKeyId</b></td><td width="65%">AKIAJI55NX5MBBCPX2XQ</td>
</tr>
<tr>
<td width="35%"><b>hive.cli.print.header</b></td><td width="65%">false</td>
</tr>
<tr>
<td width="35%"><b>javax.jdo.PersistenceManagerFactoryClass</b></td><td width="65%">org.datanucleus.jdo.JDOPersistenceManagerFactory</td>
</tr>
<tr>
<td width="35%"><b>dfs.datanode.http.address</b></td><td width="65%">0.0.0.0:50075</td>
</tr>
<tr>
<td width="35%"><b>datanucleus.validateConstraints</b></td><td width="65%">false</td>
</tr>
<tr>
<td width="35%"><b>io.sort.spill.percent</b></td><td width="65%">0.80</td>
</tr>
<tr>
<td width="35%"><b>hive.map.aggr.hash.min.reduction</b></td><td width="65%">0.5</td>
</tr>
<tr>
<td width="35%"><b>dfs.client.use.datanode.hostname</b></td><td width="65%">false</td>
</tr>
<tr>
<td width="35%"><b>mapred.job.shuffle.input.buffer.percent</b></td><td width="65%">0.70</td>
</tr>
<tr>
<td width="35%"><b>dfs.max.objects</b></td><td width="65%">0</td>
</tr>
<tr>
<td width="35%"><b>hive.exec.max.dynamic.partitions</b></td><td width="65%">1000</td>
</tr>
<tr>
<td width="35%"><b>hive.variable.substitute</b></td><td width="65%">true</td>
</tr>
<tr>
<td width="35%"><b>hadoop.security.instrumentation.requires.admin</b></td><td width="65%">false</td>
</tr>
<tr>
<td width="35%"><b>mapred.skip.map.max.skip.records</b></td><td width="65%">0</td>
</tr>
<tr>
<td width="35%"><b>mapreduce.reduce.shuffle.maxfetchfailures</b></td><td width="65%">10</td>
</tr>
<tr>
<td width="35%"><b>hadoop.security.authorization</b></td><td width="65%">false</td>
</tr>
<tr>
<td width="35%"><b>user.name</b></td><td width="65%">ubuntu</td>
</tr>
<tr>
<td width="35%"><b>hive.merge.mapredfiles</b></td><td width="65%">false</td>
</tr>
<tr>
<td width="35%"><b>hive.mapjoin.smalltable.filesize</b></td><td width="65%">25000000</td>
</tr>
<tr>
<td width="35%"><b>hive.join.emit.interval</b></td><td width="65%">1000</td>
</tr>
<tr>
<td width="35%"><b>hive.enforce.bucketing</b></td><td width="65%">false</td>
</tr>
<tr>
<td width="35%"><b>hive.optimize.ppd.storage</b></td><td width="65%">true</td>
</tr>
<tr>
<td width="35%"><b>mapred.task.profile.maps</b></td><td width="65%">0-2</td>
</tr>
<tr>
<td width="35%"><b>datanucleus.connectionPoolingType</b></td><td width="65%">DBCP</td>
</tr>
<tr>
<td width="35%"><b>dfs.https.server.keystore.resource</b></td><td width="65%">ssl-server.xml</td>
</tr>
<tr>
<td width="35%"><b>dfs.replication.interval</b></td><td width="65%">3</td>
</tr>
<tr>
<td width="35%"><b>mapred.local.dir</b></td><td width="65%">${hadoop.tmp.dir}/mapred/local</td>
</tr>
<tr>
<td width="35%"><b>mapred.merge.recordsBeforeProgress</b></td><td width="65%">10000</td>
</tr>
<tr>
<td width="35%"><b>mapred.job.tracker.http.address</b></td><td width="65%">0.0.0.0:50030</td>
</tr>
<tr>
<td width="35%"><b>hive.security.authorization.manager</b></td><td width="65%">org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider</td>
</tr>
<tr>
<td width="35%"><b>mapred.compress.map.output</b></td><td width="65%">false</td>
</tr>
<tr>
<td width="35%"><b>mapred.userlog.retain.hours</b></td><td width="65%">24</td>
</tr>
<tr>
<td width="35%"><b>hive.error.on.empty.partition</b></td><td width="65%">false</td>
</tr>
<tr>
<td width="35%"><b>mapred.tasktracker.reduce.tasks.maximum</b></td><td width="65%">2</td>
</tr>
<tr>
<td width="35%"><b>mapred.create.symlink</b></td><td width="65%">yes</td>
</tr>
<tr>
<td width="35%"><b>mapred.reducer.class</b></td><td width="65%">org.apache.hadoop.hive.ql.exec.ExecReducer</td>
</tr>
<tr>
<td width="35%"><b>javax.jdo.option.ConnectionDriverName</b></td><td width="65%">com.mysql.jdbc.Driver</td>
</tr>
<tr>
<td width="35%"><b>hadoop.security.uid.cache.secs</b></td><td width="65%">14400</td>
</tr>
<tr>
<td width="35%"><b>hive.optimize.skewjoin</b></td><td width="65%">false</td>
</tr>
<tr>
<td width="35%"><b>mapred.disk.healthChecker.interval</b></td><td width="65%">60000</td>
</tr>
<tr>
<td width="35%"><b>javax.jdo.option.DetachAllOnCommit</b></td><td width="65%">true</td>
</tr>
<tr>
<td width="35%"><b>fs.har.impl.disable.cache</b></td><td width="65%">true</td>
</tr>
<tr>
<td width="35%"><b>mapred.cluster.map.memory.mb</b></td><td width="65%">-1</td>
</tr>
<tr>
<td width="35%"><b>hive.metastore.ds.retry.interval</b></td><td width="65%">1000</td>
</tr>
<tr>
<td width="35%"><b>hadoop.job.ugi</b></td><td width="65%">root,root</td>
</tr>
<tr>
<td width="35%"><b>dfs.data.dir</b></td><td width="65%">${hadoop.tmp.dir}/dfs/data</td>
</tr>
<tr>
<td width="35%"><b>dfs.access.time.precision</b></td><td width="65%">3600000</td>
</tr>
<tr>
<td width="35%"><b>dfs.replication.min</b></td><td width="65%">1</td>
</tr>
<tr>
<td width="35%"><b>hive.exec.show.job.failure.debug.info</b></td><td width="65%">false</td>
</tr>
<tr>
<td width="35%"><b>mapreduce.job.submithost</b></td><td width="65%">ip-10-101-6-41</td>
</tr>
<tr>
<td width="35%"><b>hive.script.recordwriter</b></td><td width="65%">org.apache.hadoop.hive.ql.exec.TextRecordWriter</td>
</tr>
<tr>
<td width="35%"><b>hive.merge.size.per.task</b></td><td width="65%">256000000</td>
</tr>
<tr>
<td width="35%"><b>fs.checkpoint.dir</b></td><td width="65%">${hadoop.tmp.dir}/dfs/namesecondary</td>
</tr>
<tr>
<td width="35%"><b>javax.jdo.option.ConnectionUserName</b></td><td width="65%">hiveuser</td>
</tr>
<tr>
<td width="35%"><b>fs.s3n.impl</b></td><td width="65%">org.apache.hadoop.fs.s3native.NativeS3FileSystem</td>
</tr>
<tr>
<td width="35%"><b>mapreduce.tasktracker.outofband.heartbeat</b></td><td width="65%">false</td>
</tr>
<tr>
<td width="35%"><b>hive.join.cache.size</b></td><td width="65%">25000</td>
</tr>
<tr>
<td width="35%"><b>datanucleus.transactionIsolation</b></td><td width="65%">read-committed</td>
</tr>
<tr>
<td width="35%"><b>mapred.jobtracker.restart.recover</b></td><td width="65%">false</td>
</tr>
<tr>
<td width="35%"><b>hive.mapred.reduce.tasks.speculative.execution</b></td><td width="65%">true</td>
</tr>
<tr>
<td width="35%"><b>fs.s3.awsAccessKeyId</b></td><td width="65%">AKIAJI55NX5MBBCPX2XQ</td>
</tr>
<tr>
<td width="35%"><b>hadoop.logfile.size</b></td><td width="65%">10000000</td>
</tr>
<tr>
<td width="35%"><b>dfs.datanode.directoryscan.threads</b></td><td width="65%">1</td>
</tr>
<tr>
<td width="35%"><b>dfs.support.append</b></td><td width="65%">true</td>
</tr>
<tr>
<td width="35%"><b>mapred.inmem.merge.threshold</b></td><td width="65%">1000</td>
</tr>
<tr>
<td width="35%"><b>hive.metastore.warehouse.dir</b></td><td width="65%">/user/hive/warehouse</td>
</tr>
<tr>
<td width="35%"><b>mapred.max.split.size</b></td><td width="65%">256000000</td>
</tr>
<tr>
<td width="35%"><b>ipc.client.connection.maxidletime</b></td><td width="65%">10000</td>
</tr>
<tr>
<td width="35%"><b>hive.exec.parallel.thread.number</b></td><td width="65%">8</td>
</tr>
<tr>
<td width="35%"><b>hive.zookeeper.namespace</b></td><td width="65%">hive_zookeeper_namespace</td>
</tr>
<tr>
<td width="35%"><b>fs.checkpoint.size</b></td><td width="65%">67108864</td>
</tr>
<tr>
<td width="35%"><b>hive.map.aggr.hash.force.flush.memory.threshold</b></td><td width="65%">0.9</td>
</tr>
<tr>
<td width="35%"><b>mapred.partitioner.class</b></td><td width="65%">org.apache.hadoop.hive.ql.io.DefaultHivePartitioner</td>
</tr>
<tr>
<td width="35%"><b>dfs.blockreport.intervalMsec</b></td><td width="65%">3600000</td>
</tr>
<tr>
<td width="35%"><b>fs.s3.sleepTimeSeconds</b></td><td width="65%">10</td>
</tr>
<tr>
<td width="35%"><b>mapreduce.job.counters.counter.name.max</b></td><td width="65%">64</td>
</tr>
<tr>
<td width="35%"><b>dfs.client.block.write.retries</b></td><td width="65%">3</td>
</tr>
<tr>
<td width="35%"><b>hadoop.config.dir</b></td><td width="65%">/home/ubuntu/.whirr/dron</td>
</tr>
<tr>
<td width="35%"><b>dfs.namenode.name.dir.restore</b></td><td width="65%">false</td>
</tr>
<tr>
<td width="35%"><b>mapred.reduce.tasks</b></td><td width="65%">1</td>
</tr>
<tr>
<td width="35%"><b>mapred.queue.names</b></td><td width="65%">default</td>
</tr>
<tr>
<td width="35%"><b>hive.exec.drop.ignorenonexistent</b></td><td width="65%">true</td>
</tr>
<tr>
<td width="35%"><b>io.seqfile.lazydecompress</b></td><td width="65%">true</td>
</tr>
<tr>
<td width="35%"><b>dfs.https.enable</b></td><td width="65%">false</td>
</tr>
<tr>
<td width="35%"><b>hive.test.mode.prefix</b></td><td width="65%">test_</td>
</tr>
<tr>
<td width="35%"><b>hive.udtf.auto.progress</b></td><td width="65%">false</td>
</tr>
<tr>
<td width="35%"><b>dfs.replication</b></td><td width="65%">3</td>
</tr>
<tr>
<td width="35%"><b>ipc.client.tcpnodelay</b></td><td width="65%">false</td>
</tr>
<tr>
<td width="35%"><b>hive.optimize.reducededuplication</b></td><td width="65%">true</td>
</tr>
<tr>
<td width="35%"><b>hive.query.id</b></td><td width="65%">ubuntu_20120525214141_c6de4f27-c640-4afc-a5d2-0a62447d324f</td>
</tr>
<tr>
<td width="35%"><b>mapred.output.format.class</b></td><td width="65%">org.apache.hadoop.mapred.lib.NullOutputFormat</td>
</tr>
<tr>
<td width="35%"><b>mapred.acls.enabled</b></td><td width="65%">false</td>
</tr>
<tr>
<td width="35%"><b>mapred.tasktracker.dns.nameserver</b></td><td width="65%">default</td>
</tr>
<tr>
<td width="35%"><b>hive.mapjoin.localtask.max.memory.usage</b></td><td width="65%">0.90</td>
</tr>
<tr>
<td width="35%"><b>mapred.submit.replication</b></td><td width="65%">10</td>
</tr>
<tr>
<td width="35%"><b>hive.mapjoin.followby.map.aggr.hash.percentmemory</b></td><td width="65%">0.3</td>
</tr>
<tr>
<td width="35%"><b>io.compression.codecs</b></td><td width="65%">org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.BZip2Codec,org.apache.hadoop.io.compress.DeflateCodec,org.apache.hadoop.io.compress.SnappyCodec</td>
</tr>
<tr>
<td width="35%"><b>hive.exec.script.maxerrsize</b></td><td width="65%">100000</td>
</tr>
<tr>
<td width="35%"><b>io.file.buffer.size</b></td><td width="65%">4096</td>
</tr>
<tr>
<td width="35%"><b>mapred.map.tasks.speculative.execution</b></td><td width="65%">true</td>
</tr>
<tr>
<td width="35%"><b>mapred.map.max.attempts</b></td><td width="65%">4</td>
</tr>
<tr>
<td width="35%"><b>mapred.job.shuffle.merge.percent</b></td><td width="65%">0.66</td>
</tr>
<tr>
<td width="35%"><b>fs.har.impl</b></td><td width="65%">org.apache.hadoop.hive.shims.HiveHarFileSystem</td>
</tr>
<tr>
<td width="35%"><b>hadoop.security.authentication</b></td><td width="65%">simple</td>
</tr>
<tr>
<td width="35%"><b>fs.s3.buffer.dir</b></td><td width="65%">${hadoop.tmp.dir}/s3</td>
</tr>
<tr>
<td width="35%"><b>mapred.skip.reduce.auto.incr.proc.count</b></td><td width="65%">true</td>
</tr>
<tr>
<td width="35%"><b>dfs.http.address</b></td><td width="65%">0.0.0.0:50070</td>
</tr>
<tr>
<td width="35%"><b>mapred.job.tracker.jobhistory.lru.cache.size</b></td><td width="65%">5</td>
</tr>
<tr>
<td width="35%"><b>hive.optimize.groupby</b></td><td width="65%">true</td>
</tr>
<tr>
<td width="35%"><b>dfs.replication.considerLoad</b></td><td width="65%">true</td>
</tr>
<tr>
<td width="35%"><b>dfs.block.access.token.enable</b></td><td width="65%">false</td>
</tr>
<tr>
<td width="35%"><b>mapreduce.job.acl-view-job</b></td><td width="65%"> </td>
</tr>
<tr>
<td width="35%"><b>javax.jdo.option.NonTransactionalRead</b></td><td width="65%">true</td>
</tr>
<tr>
<td width="35%"><b>mapred.job.queue.name</b></td><td width="65%">default</td>
</tr>
<tr>
<td width="35%"><b>hive.metastore.connect.retries</b></td><td width="65%">5</td>
</tr>
<tr>
<td width="35%"><b>dfs.permissions</b></td><td width="65%">true</td>
</tr>
<tr>
<td width="35%"><b>mapred.job.tracker.persist.jobstatus.hours</b></td><td width="65%">0</td>
</tr>
<tr>
<td width="35%"><b>hive.exec.dynamic.partition</b></td><td width="65%">false</td>
</tr>
<tr>
<td width="35%"><b>hive.stats.dbconnectionstring</b></td><td width="65%">jdbc:derby:;databaseName=TempStatsStore;create=true</td>
</tr>
<tr>
<td width="35%"><b>mapreduce.tasktracker.cache.local.numberdirectories</b></td><td width="65%">10000</td>
</tr>
<tr>
<td width="35%"><b>fs.file.impl</b></td><td width="65%">org.apache.hadoop.fs.LocalFileSystem</td>
</tr>
<tr>
<td width="35%"><b>dfs.block.size</b></td><td width="65%">67108864</td>
</tr>
<tr>
<td width="35%"><b>hive.security.authenticator.manager</b></td><td width="65%">org.apache.hadoop.hive.ql.security.HadoopDefaultAuthenticator</td>
</tr>
<tr>
<td width="35%"><b>hive.stats.jdbc.atomic</b></td><td width="65%">false</td>
</tr>
<tr>
<td width="35%"><b>hadoop.socks.server</b></td><td width="65%">localhost:6666</td>
</tr>
<tr>
<td width="35%"><b>datanucleus.fixedDatastore</b></td><td width="65%">true</td>
</tr>
<tr>
<td width="35%"><b>dfs.https.address</b></td><td width="65%">0.0.0.0:50470</td>
</tr>
<tr>
<td width="35%"><b>ipc.client.kill.max</b></td><td width="65%">10</td>
</tr>
<tr>
<td width="35%"><b>mapred.healthChecker.script.timeout</b></td><td width="65%">600000</td>
</tr>
<tr>
<td width="35%"><b>hive.lock.sleep.between.retries</b></td><td width="65%">60</td>
</tr>
<tr>
<td width="35%"><b>hive.exec.reducers.max</b></td><td width="65%">999</td>
</tr>
<tr>
<td width="35%"><b>mapred.tasktracker.map.tasks.maximum</b></td><td width="65%">2</td>
</tr>
<tr>
<td width="35%"><b>datanucleus.cache.level2</b></td><td width="65%">false</td>
</tr>
<tr>
<td width="35%"><b>jobclient.completion.poll.interval</b></td><td width="65%">5000</td>
</tr>
<tr>
<td width="35%"><b>mapred.job.tracker.persist.jobstatus.dir</b></td><td width="65%">/jobtracker/jobsInfo</td>
</tr>
<tr>
<td width="35%"><b>dfs.default.chunk.view.size</b></td><td width="65%">32768</td>
</tr>
<tr>
<td width="35%"><b>mapred.reduce.slowstart.completed.maps</b></td><td width="65%">0.05</td>
</tr>
<tr>
<td width="35%"><b>hive.security.authorization.enabled</b></td><td width="65%">false</td>
</tr>
<tr>
<td width="35%"><b>mapred.mapper.class</b></td><td width="65%">org.apache.hadoop.hive.ql.exec.ExecMapper</td>
</tr>
<tr>
<td width="35%"><b>io.sort.mb</b></td><td width="65%">100</td>
</tr>
<tr>
<td width="35%"><b>dfs.datanode.failed.volumes.tolerated</b></td><td width="65%">0</td>
</tr>
<tr>
<td width="35%"><b>dfs.https.need.client.auth</b></td><td width="65%">false</td>
</tr>
<tr>
<td width="35%"><b>hadoop.http.authentication.type</b></td><td width="65%">simple</td>
</tr>
<tr>
<td width="35%"><b>dfs.datanode.data.dir.perm</b></td><td width="65%">700</td>
</tr>
<tr>
<td width="35%"><b>ipc.server.listen.queue.size</b></td><td width="65%">128</td>
</tr>
<tr>
<td width="35%"><b>hive.optimize.pruner</b></td><td width="65%">true</td>
</tr>
<tr>
<td width="35%"><b>hive.script.operator.id.env.var</b></td><td width="65%">HIVE_SCRIPT_OPERATOR_ID</td>
</tr>
<tr>
<td width="35%"><b>io.mapfile.bloom.size</b></td><td width="65%">1048576</td>
</tr>
<tr>
<td width="35%"><b>fs.hsftp.impl</b></td><td width="65%">org.apache.hadoop.hdfs.HsftpFileSystem</td>
</tr>
<tr>
<td width="35%"><b>hive.metastore.rawstore.impl</b></td><td width="65%">org.apache.hadoop.hive.metastore.ObjectStore</td>
</tr>
<tr>
<td width="35%"><b>mapred.cache.files.timestamps</b></td><td width="65%">1337982960284</td>
</tr>
<tr>
<td width="35%"><b>hive.mapjoin.hashtable.initialCapacity</b></td><td width="65%">100000</td>
</tr>
<tr>
<td width="35%"><b>dfs.datanode.dns.nameserver</b></td><td width="65%">default</td>
</tr>
<tr>
<td width="35%"><b>hive.concurrency.manager</b></td><td width="65%">org.apache.hadoop.hive.ql.lockmgr.ZooKeeperLockMgr</td>
</tr>
<tr>
<td width="35%"><b>mapred.child.java.opts</b></td><td width="65%">-Xmx200m</td>
</tr>
<tr>
<td width="35%"><b>dfs.replication.max</b></td><td width="65%">512</td>
</tr>
<tr>
<td width="35%"><b>mapred.queue.default.state</b></td><td width="65%">RUNNING</td>
</tr>
<tr>
<td width="35%"><b>map.sort.class</b></td><td width="65%">org.apache.hadoop.util.QuickSort</td>
</tr>
<tr>
<td width="35%"><b>hive.exec.dynamic.partition.mode</b></td><td width="65%">strict</td>
</tr>
<tr>
<td width="35%"><b>mapred.jobtracker.instrumentation</b></td><td width="65%">org.apache.hadoop.mapred.JobTrackerMetricsInst</td>
</tr>
<tr>
<td width="35%"><b>hadoop.util.hash.type</b></td><td width="65%">murmur</td>
</tr>
<tr>
<td width="35%"><b>topology.node.switch.mapping.impl</b></td><td width="65%">org.apache.hadoop.net.ScriptBasedMapping</td>
</tr>
<tr>
<td width="35%"><b>dfs.block.access.key.update.interval</b></td><td width="65%">600</td>
</tr>
<tr>
<td width="35%"><b>dfs.datanode.dns.interface</b></td><td width="65%">default</td>
</tr>
<tr>
<td width="35%"><b>dfs.datanode.use.datanode.hostname</b></td><td width="65%">false</td>
</tr>
<tr>
<td width="35%"><b>hive.fetch.output.serde</b></td><td width="65%">org.apache.hadoop.hive.serde2.DelimitedJSONSerDe</td>
</tr>
<tr>
<td width="35%"><b>mapred.output.compression.type</b></td><td width="65%">RECORD</td>
</tr>
<tr>
<td width="35%"><b>mapred.skip.attempts.to.start.skipping</b></td><td width="65%">2</td>
</tr>
<tr>
<td width="35%"><b>mapreduce.job.dir</b></td><td width="65%">hdfs://ec2-50-19-196-189.compute-1.amazonaws.com/user/ubuntu/.staging/job_201205252023_0038</td>
</tr>
<tr>
<td width="35%"><b>io.map.index.skip</b></td><td width="65%">0</td>
</tr>
<tr>
<td width="35%"><b>hive.enforce.sorting</b></td><td width="65%">false</td>
</tr>
<tr>
<td width="35%"><b>hive.metastore.server.tcp.keepalive</b></td><td width="65%">true</td>
</tr>
<tr>
<td width="35%"><b>mapred.cluster.max.map.memory.mb</b></td><td width="65%">-1</td>
</tr>
<tr>
<td width="35%"><b>fs.s3.maxRetries</b></td><td width="65%">4</td>
</tr>
<tr>
<td width="35%"><b>dfs.namenode.logging.level</b></td><td width="65%">info</td>
</tr>
<tr>
<td width="35%"><b>mapred.task.tracker.task-controller</b></td><td width="65%">org.apache.hadoop.mapred.DefaultTaskController</td>
</tr>
<tr>
<td width="35%"><b>mapred.userlog.limit.kb</b></td><td width="65%">0</td>
</tr>
<tr>
<td width="35%"><b>hive.script.auto.progress</b></td><td width="65%">false</td>
</tr>
<tr>
<td width="35%"><b>mapred.input.format.class</b></td><td width="65%">org.apache.hadoop.hive.ql.io.CombineHiveInputFormat</td>
</tr>
<tr>
<td width="35%"><b>hive.mapred.local.mem</b></td><td width="65%">0</td>
</tr>
<tr>
<td width="35%"><b>hive.metastore.kerberos.principal</b></td><td width="65%">hive-metastore/_HOST@EXAMPLE.COM</td>
</tr>
<tr>
<td width="35%"><b>hadoop.http.authentication.simple.anonymous.allowed</b></td><td width="65%">true</td>
</tr>
<tr>
<td width="35%"><b>hive.optimize.ppd</b></td><td width="65%">true</td>
</tr>
<tr>
<td width="35%"><b>hadoop.rpc.socket.factory.class.default</b></td><td width="65%">org.apache.hadoop.net.StandardSocketFactory</td>
</tr>
<tr>
<td width="35%"><b>hive.exec.plan</b></td><td width="65%">hdfs://ec2-50-19-196-189.compute-1.amazonaws.com/tmp/hive-ubuntu/hive_2012-05-25_21-41-26_100_8603489155441756514/-mr-10012/9169bb19-80ce-4c9a-9209-6009098061e8</td>
</tr>
<tr>
<td width="35%"><b>fs.hftp.impl</b></td><td width="65%">org.apache.hadoop.hdfs.HftpFileSystem</td>
</tr>
<tr>
<td width="35%"><b>hive.exec.parallel</b></td><td width="65%">false</td>
</tr>
<tr>
<td width="35%"><b>dfs.namenode.handler.count</b></td><td width="65%">10</td>
</tr>
<tr>
<td width="35%"><b>hive.exec.max.created.files</b></td><td width="65%">100000</td>
</tr>
<tr>
<td width="35%"><b>fs.automatic.close</b></td><td width="65%">true</td>
</tr>
<tr>
<td width="35%"><b>fs.kfs.impl</b></td><td width="65%">org.apache.hadoop.fs.kfs.KosmosFileSystem</td>
</tr>
<tr>
<td width="35%"><b>mapreduce.job.submithostaddress</b></td><td width="65%">127.0.1.1</td>
</tr>
<tr>
<td width="35%"><b>hive.stats.jdbcdriver</b></td><td width="65%">org.apache.derby.jdbc.EmbeddedDriver</td>
</tr>
<tr>
<td width="35%"><b>mapred.committer.job.setup.cleanup.needed</b></td><td width="65%">false</td>
</tr>
<tr>
<td width="35%"><b>mapred.map.tasks</b></td><td width="65%">1</td>
</tr>
<tr>
<td width="35%"><b>hive.script.recordreader</b></td><td width="65%">org.apache.hadoop.hive.ql.exec.TextRecordReader</td>
</tr>
<tr>
<td width="35%"><b>mapred.local.dir.minspacekill</b></td><td width="65%">0</td>
</tr>
<tr>
<td width="35%"><b>fs.hdfs.impl</b></td><td width="65%">org.apache.hadoop.hdfs.DistributedFileSystem</td>
</tr>
<tr>
<td width="35%"><b>mapred.job.map.memory.mb</b></td><td width="65%">-1</td>
</tr>
<tr>
<td width="35%"><b>hive.mergejob.maponly</b></td><td width="65%">true</td>
</tr>
<tr>
<td width="35%"><b>mapred.jobtracker.completeuserjobs.maximum</b></td><td width="65%">100</td>
</tr>
<tr>
<td width="35%"><b>mapreduce.jobtracker.split.metainfo.maxsize</b></td><td width="65%">10000000</td>
</tr>
<tr>
<td width="35%"><b>hive.auto.progress.timeout</b></td><td width="65%">0</td>
</tr>
<tr>
<td width="35%"><b>jobclient.progress.monitor.poll.interval</b></td><td width="65%">1000</td>
</tr>
<tr>
<td width="35%"><b>javax.jdo.option.ConnectionPassword</b></td><td width="65%">HIVE</td>
</tr>
<tr>
<td width="35%"><b>hive.mapjoin.cache.numrows</b></td><td width="65%">25000</td>
</tr>
<tr>
<td width="35%"><b>dfs.blockreport.initialDelay</b></td><td width="65%">0</td>
</tr>
<tr>
<td width="35%"><b>mapred.min.split.size</b></td><td width="65%">1</td>
</tr>
<tr>
<td width="35%"><b>hadoop.http.authentication.token.validity</b></td><td width="65%">36000</td>
</tr>
<tr>
<td width="35%"><b>hive.groupby.mapaggr.checkinterval</b></td><td width="65%">100000</td>
</tr>
<tr>
<td width="35%"><b>dfs.namenode.delegation.token.max-lifetime</b></td><td width="65%">604800000</td>
</tr>
<tr>
<td width="35%"><b>fs.ftp.impl</b></td><td width="65%">org.apache.hadoop.fs.ftp.FTPFileSystem</td>
</tr>
<tr>
<td width="35%"><b>dfs.secondary.http.address</b></td><td width="65%">0.0.0.0:50090</td>
</tr>
<tr>
<td width="35%"><b>mapred.output.compression.codec</b></td><td width="65%">org.apache.hadoop.io.compress.DefaultCodec</td>
</tr>
<tr>
<td width="35%"><b>mapred.cache.files</b></td><td width="65%">hdfs://ec2-50-19-196-189.compute-1.amazonaws.com/tmp/hive-ubuntu/hive_2012-05-25_21-41-26_100_8603489155441756514/-mr-10012/9169bb19-80ce-4c9a-9209-6009098061e8#HIVE_PLAN9169bb19-80ce-4c9a-9209-6009098061e8</td>
</tr>
<tr>
<td width="35%"><b>hive.metastore.cache.pinobjtypes</b></td><td width="65%">Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order</td>
</tr>
<tr>
<td width="35%"><b>mapred.cluster.max.reduce.memory.mb</b></td><td width="65%">-1</td>
</tr>
<tr>
<td width="35%"><b>mapred.cluster.reduce.memory.mb</b></td><td width="65%">-1</td>
</tr>
<tr>
<td width="35%"><b>dfs.web.ugi</b></td><td width="65%">webuser,webgroup</td>
</tr>
<tr>
<td width="35%"><b>mapred.task.profile</b></td><td width="65%">false</td>
</tr>
<tr>
<td width="35%"><b>mapred.reduce.parallel.copies</b></td><td width="65%">5</td>
</tr>
<tr>
<td width="35%"><b>dfs.heartbeat.interval</b></td><td width="65%">3</td>
</tr>
<tr>
<td width="35%"><b>datanucleus.plugin.pluginRegistryBundleCheck</b></td><td width="65%">LOG</td>
</tr>
<tr>
<td width="35%"><b>hive.mapjoin.hashtable.loadfactor</b></td><td width="65%">0.75</td>
</tr>
<tr>
<td width="35%"><b>hive.metastore.server.max.threads</b></td><td width="65%">100000</td>
</tr>
<tr>
<td width="35%"><b>local.cache.size</b></td><td width="65%">10737418240</td>
</tr>
<tr>
<td width="35%"><b>hive.archive.har.parentdir.settable</b></td><td width="65%">false</td>
</tr>
<tr>
<td width="35%"><b>hive.exec.reducers.bytes.per.reducer</b></td><td width="65%">1000000000</td>
</tr>
<tr>
<td width="35%"><b>hive.hwi.listen.host</b></td><td width="65%">0.0.0.0</td>
</tr>
<tr>
<td width="35%"><b>hive.exec.mode.local.auto</b></td><td width="65%">false</td>
</tr>
<tr>
<td width="35%"><b>io.sort.factor</b></td><td width="65%">10</td>
</tr>
<tr>
<td width="35%"><b>mapred.input.dir</b></td><td width="65%">hdfs://ec2-50-19-196-189.compute-1.amazonaws.com/tmp/hive-ubuntu/hive_2012-05-25_21-41-26_100_8603489155441756514/-mr-10004</td>
</tr>
<tr>
<td width="35%"><b>mapreduce.job.counters.groups.max</b></td><td width="65%">50</td>
</tr>
<tr>
<td width="35%"><b>mapred.task.timeout</b></td><td width="65%">600000</td>
</tr>
<tr>
<td width="35%"><b>hive.metastore.client.connect.retry.delay</b></td><td width="65%">1</td>
</tr>
<tr>
<td width="35%"><b>hive.archive.enabled</b></td><td width="65%">false</td>
</tr>
<tr>
<td width="35%"><b>hive.metastore.local</b></td><td width="65%">true</td>
</tr>
<tr>
<td width="35%"><b>datanucleus.autoCreateSchema</b></td><td width="65%">false</td>
</tr>
<tr>
<td width="35%"><b>dfs.safemode.extension</b></td><td width="65%">30000</td>
</tr>
<tr>
<td width="35%"><b>hive.merge.mapfiles</b></td><td width="65%">true</td>
</tr>
<tr>
<td width="35%"><b>ipc.client.idlethreshold</b></td><td width="65%">4000</td>
</tr>
<tr>
<td width="35%"><b>ipc.server.tcpnodelay</b></td><td width="65%">false</td>
</tr>
<tr>
<td width="35%"><b>hadoop.logfile.count</b></td><td width="65%">10</td>
</tr>
<tr>
<td width="35%"><b>hive.zookeeper.clean.extra.nodes</b></td><td width="65%">false</td>
</tr>
<tr>
<td width="35%"><b>group.name</b></td><td width="65%">ubuntu</td>
</tr>
<tr>
<td width="35%"><b>hive.support.concurrency</b></td><td width="65%">false</td>
</tr>
<tr>
<td width="35%"><b>hive.exec.default.partition.name</b></td><td width="65%">__HIVE_DEFAULT_PARTITION__</td>
</tr>
<tr>
<td width="35%"><b>mapred.heartbeats.in.second</b></td><td width="65%">100</td>
</tr>
<tr>
<td width="35%"><b>fs.s3.block.size</b></td><td width="65%">67108864</td>
</tr>
<tr>
<td width="35%"><b>hive.exec.counters.pull.interval</b></td><td width="65%">1000</td>
</tr>
<tr>
<td width="35%"><b>mapred.map.output.compression.codec</b></td><td width="65%">org.apache.hadoop.io.compress.DefaultCodec</td>
</tr>
<tr>
<td width="35%"><b>mapred.task.cache.levels</b></td><td width="65%">2</td>
</tr>
<tr>
<td width="35%"><b>hive.groupby.skewindata</b></td><td width="65%">false</td>
</tr>
<tr>
<td width="35%"><b>mapred.tasktracker.dns.interface</b></td><td width="65%">default</td>
</tr>
<tr>
<td width="35%"><b>mapred.output.key.class</b></td><td width="65%">org.apache.hadoop.io.Text</td>
</tr>
<tr>
<td width="35%"><b>datanucleus.cache.level2.type</b></td><td width="65%">SOFT</td>
</tr>
<tr>
<td width="35%"><b>dfs.safemode.min.datanodes</b></td><td width="65%">0</td>
</tr>
<tr>
<td width="35%"><b>mapred.job.reduce.memory.mb</b></td><td width="65%">-1</td>
</tr>
<tr>
<td width="35%"><b>hive.zookeeper.client.port</b></td><td width="65%">2181</td>
</tr>
<tr>
<td width="35%"><b>mapred.mapoutput.value.class</b></td><td width="65%">org.apache.hadoop.io.BytesWritable</td>
</tr>
<tr>
<td width="35%"><b>mapred.max.tracker.failures</b></td><td width="65%">4</td>
</tr>
<tr>
<td width="35%"><b>hadoop.http.authentication.signature.secret.file</b></td><td width="65%">${user.home}/hadoop-http-auth-signature-secret</td>
</tr>
<tr>
<td width="35%"><b>dfs.df.interval</b></td><td width="65%">60000</td>
</tr>
<tr>
<td width="35%"><b>hive.mapjoin.check.memory.rows</b></td><td width="65%">100000</td>
</tr>
<tr>
<td width="35%"><b>mapreduce.reduce.shuffle.read.timeout</b></td><td width="65%">180000</td>
</tr>
<tr>
<td width="35%"><b>mapred.tasktracker.tasks.sleeptime-before-sigkill</b></td><td width="65%">5000</td>
</tr>
<tr>
<td width="35%"><b>hive.index.compact.file.ignore.hdfs</b></td><td width="65%">false</td>
</tr>
<tr>
<td width="35%"><b>hive.task.progress</b></td><td width="65%">false</td>
</tr>
<tr>
<td width="35%"><b>hive.stats.dbclass</b></td><td width="65%">jdbc:derby</td>
</tr>
<tr>
<td width="35%"><b>datanucleus.storeManagerType</b></td><td width="65%">rdbms</td>
</tr>
<tr>
<td width="35%"><b>mapred.max.tracker.blacklists</b></td><td width="65%">4</td>
</tr>
<tr>
<td width="35%"><b>hive.session.id</b></td><td width="65%">ubuntu_201205252141</td>
</tr>
<tr>
<td width="35%"><b>hive.optimize.cp</b></td><td width="65%">true</td>
</tr>
<tr>
<td width="35%"><b>jobclient.output.filter</b></td><td width="65%">FAILED</td>
</tr>
<tr>
<td width="35%"><b>io.serializations</b></td><td width="65%">org.apache.hadoop.io.serializer.WritableSerialization</td>
</tr>
<tr>
<td width="35%"><b>io.seqfile.compress.blocksize</b></td><td width="65%">1000000</td>
</tr>
<tr>
<td width="35%"><b>mapred.jobtracker.taskScheduler</b></td><td width="65%">org.apache.hadoop.mapred.JobQueueTaskScheduler</td>
</tr>
<tr>
<td width="35%"><b>job.end.retry.attempts</b></td><td width="65%">0</td>
</tr>
<tr>
<td width="35%"><b>ipc.client.connect.max.retries</b></td><td width="65%">10</td>
</tr>
<tr>
<td width="35%"><b>dfs.namenode.delegation.key.update-interval</b></td><td width="65%">86400000</td>
</tr>
<tr>
<td width="35%"><b>webinterface.private.actions</b></td><td width="65%">false</td>
</tr>
<tr>
<td width="35%"><b>hive.map.aggr</b></td><td width="65%">true</td>
</tr>
<tr>
<td width="35%"><b>mapred.tasktracker.indexcache.mb</b></td><td width="65%">10</td>
</tr>
<tr>
<td width="35%"><b>fs.checkpoint.edits.dir</b></td><td width="65%">${fs.checkpoint.dir}</td>
</tr>
<tr>
<td width="35%"><b>mapreduce.reduce.input.limit</b></td><td width="65%">-1</td>
</tr>
<tr>
<td width="35%"><b>hive.test.mode.samplefreq</b></td><td width="65%">32</td>
</tr>
<tr>
<td width="35%"><b>hive.exec.scratchdir</b></td><td width="65%">/tmp/hive-${user.name}</td>
</tr>
<tr>
<td width="35%"><b>datanucleus.validateTables</b></td><td width="65%">false</td>
</tr>
<tr>
<td width="35%"><b>mapred.output.value.class</b></td><td width="65%">org.apache.hadoop.io.Text</td>
</tr>
<tr>
<td width="35%"><b>tasktracker.http.threads</b></td><td width="65%">40</td>
</tr>
<tr>
<td width="35%"><b>hive.exec.max.dynamic.partitions.pernode</b></td><td width="65%">100</td>
</tr>
<tr>
<td width="35%"><b>fs.s3n.block.size</b></td><td width="65%">67108864</td>
</tr>
<tr>
<td width="35%"><b>mapreduce.job.counters.group.name.max</b></td><td width="65%">128</td>
</tr>
<tr>
<td width="35%"><b>mapred.job.tracker.handler.count</b></td><td width="65%">10</td>
</tr>
<tr>
<td width="35%"><b>keep.failed.task.files</b></td><td width="65%">false</td>
</tr>
<tr>
<td width="35%"><b>mapred.output.compress</b></td><td width="65%">false</td>
</tr>
<tr>
<td width="35%"><b>hadoop.security.group.mapping</b></td><td width="65%">org.apache.hadoop.security.ShellBasedUnixGroupsMapping</td>
</tr>
<tr>
<td width="35%"><b>mapred.cache.files.filesizes</b></td><td width="65%">25532</td>
</tr>
<tr>
<td width="35%"><b>dfs.https.client.keystore.resource</b></td><td width="65%">ssl-client.xml</td>
</tr>
<tr>
<td width="35%"><b>mapred.jobtracker.job.history.block.size</b></td><td width="65%">3145728</td>
</tr>
<tr>
<td width="35%"><b>mapred.skip.reduce.max.skip.groups</b></td><td width="65%">0</td>
</tr>
<tr>
<td width="35%"><b>hive.stats.autogather</b></td><td width="65%">true</td>
</tr>
<tr>
<td width="35%"><b>dfs.datanode.address</b></td><td width="65%">0.0.0.0:50010</td>
</tr>
<tr>
<td width="35%"><b>hive.rework.mapredwork</b></td><td width="65%">false</td>
</tr>
<tr>
<td width="35%"><b>dfs.datanode.https.address</b></td><td width="65%">0.0.0.0:50475</td>
</tr>
<tr>
<td width="35%"><b>fs.s3.impl</b></td><td width="65%">org.apache.hadoop.fs.s3.S3FileSystem</td>
</tr>
<tr>
<td width="35%"><b>hive.exec.script.allow.partial.consumption</b></td><td width="65%">false</td>
</tr>
<tr>
<td width="35%"><b>hive.auto.convert.join</b></td><td width="65%">false</td>
</tr>
<tr>
<td width="35%"><b>mapred.jar</b></td><td width="65%">/user/ubuntu/.staging/job_201205252023_0038/job.jar</td>
</tr>
<tr>
<td width="35%"><b>hive.exec.compress.output</b></td><td width="65%">false</td>
</tr>
<tr>
<td width="35%"><b>hadoop.tmp.dir</b></td><td width="65%">/tmp/hadoop-${user.name}</td>
</tr>
<tr>
<td width="35%"><b>mapred.line.input.format.linespermap</b></td><td width="65%">1</td>
</tr>
<tr>
<td width="35%"><b>hadoop.kerberos.kinit.command</b></td><td width="65%">kinit</td>
</tr>
<tr>
<td width="35%"><b>mapred.min.split.size.per.node</b></td><td width="65%">1</td>
</tr>
<tr>
<td width="35%"><b>dfs.datanode.du.reserved</b></td><td width="65%">0</td>
</tr>
<tr>
<td width="35%"><b>topology.script.number.args</b></td><td width="65%">100</td>
</tr>
<tr>
<td width="35%"><b>fs.default.name</b></td><td width="65%">hdfs://ec2-50-19-196-189.compute-1.amazonaws.com:8020/</td>
</tr>
<tr>
<td width="35%"><b>dfs.balance.bandwidthPerSec</b></td><td width="65%">1048576</td>
</tr>
<tr>
<td width="35%"><b>hive.metastore.sasl.enabled</b></td><td width="65%">false</td>
</tr>
<tr>
<td width="35%"><b>mapred.local.dir.minspacestart</b></td><td width="65%">0</td>
</tr>
<tr>
<td width="35%"><b>mapred.jobtracker.maxtasks.per.job</b></td><td width="65%">-1</td>
</tr>
<tr>
<td width="35%"><b>hive.hbase.wal.enabled</b></td><td width="65%">true</td>
</tr>
<tr>
<td width="35%"><b>hive.test.mode</b></td><td width="65%">false</td>
</tr>
<tr>
<td width="35%"><b>mapred.user.jobconf.limit</b></td><td width="65%">5242880</td>
</tr>
<tr>
<td width="35%"><b>mapred.reduce.max.attempts</b></td><td width="65%">4</td>
</tr>
<tr>
<td width="35%"><b>datanucleus.validateColumns</b></td><td width="65%">false</td>
</tr>
<tr>
<td width="35%"><b>mapred.job.tracker</b></td><td width="65%">ec2-50-19-196-189.compute-1.amazonaws.com:8021</td>
</tr>
<tr>
<td width="35%"><b>dfs.namenode.decommission.interval</b></td><td width="65%">30</td>
</tr>
<tr>
<td width="35%"><b>hive.mapjoin.bucket.cache.size</b></td><td width="65%">100</td>
</tr>
<tr>
<td width="35%"><b>dfs.name.edits.dir</b></td><td width="65%">${dfs.name.dir}</td>
</tr>
<tr>
<td width="35%"><b>mapred.output.committer.class</b></td><td width="65%">org.apache.hadoop.hive.shims.Hadoop20Shims$NullOutputCommitter</td>
</tr>
<tr>
<td width="35%"><b>hive.metastore.client.socket.timeout</b></td><td width="65%">20</td>
</tr>
<tr>
<td width="35%"><b>mapred.tasktracker.instrumentation</b></td><td width="65%">org.apache.hadoop.mapred.TaskTrackerMetricsInst</td>
</tr>
<tr>
<td width="35%"><b>io.mapfile.bloom.error.rate</b></td><td width="65%">0.005</td>
</tr>
<tr>
<td width="35%"><b>mapred.tasktracker.expiry.interval</b></td><td width="65%">600000</td>
</tr>
<tr>
<td width="35%"><b>io.sort.record.percent</b></td><td width="65%">0.05</td>
</tr>
<tr>
<td width="35%"><b>dfs.safemode.threshold.pct</b></td><td width="65%">0.999f</td>
</tr>
<tr>
<td width="35%"><b>hive.map.aggr.hash.percentmemory</b></td><td width="65%">0.5</td>
</tr>
<tr>
<td width="35%"><b>mapred.job.tracker.persist.jobstatus.active</b></td><td width="65%">false</td>
</tr>
<tr>
<td width="35%"><b>dfs.name.dir</b></td><td width="65%">${hadoop.tmp.dir}/dfs/name</td>
</tr>
<tr>
<td width="35%"><b>mapreduce.job.acl-modify-job</b></td><td width="65%"> </td>
</tr>
<tr>
<td width="35%"><b>fs.checkpoint.period</b></td><td width="65%">3600</td>
</tr>
<tr>
<td width="35%"><b>hive.exec.compress.intermediate</b></td><td width="65%">false</td>
</tr>
<tr>
<td width="35%"><b>hive.lock.numretries</b></td><td width="65%">100</td>
</tr>
<tr>
<td width="35%"><b>hive.default.fileformat</b></td><td width="65%">TextFile</td>
</tr>
<tr>
<td width="35%"><b>io.skip.checksum.errors</b></td><td width="65%">false</td>
</tr>
<tr>
<td width="35%"><b>dfs.datanode.handler.count</b></td><td width="65%">3</td>
</tr>
<tr>
<td width="35%"><b>dfs.namenode.decommission.nodes.per.interval</b></td><td width="65%">5</td>
</tr>
<tr>
<td width="35%"><b>mapred.temp.dir</b></td><td width="65%">${hadoop.tmp.dir}/mapred/temp</td>
</tr>
<tr>
<td width="35%"><b>hive.fileformat.check</b></td><td width="65%">true</td>
</tr>
<tr>
<td width="35%"><b>fs.s3n.awsSecretAccessKey</b></td><td width="65%">cEiOvcieTmWFBhU/TosMmn+6vNwiXn+8KbcO0D6G</td>
</tr>
<tr>
<td width="35%"><b>mapred.mapoutput.key.class</b></td><td width="65%">org.apache.hadoop.hive.ql.io.HiveKey</td>
</tr>
<tr>
<td width="35%"><b>hadoop.native.lib</b></td><td width="65%">true</td>
</tr>
<tr>
<td width="35%"><b>hive.merge.smallfiles.avgsize</b></td><td width="65%">16000000</td>
</tr>
<tr>
<td width="35%"><b>mapreduce.job.counters.limit</b></td><td width="65%">120</td>
</tr>
<tr>
<td width="35%"><b>dfs.datanode.ipc.address</b></td><td width="65%">0.0.0.0:50020</td>
</tr>
<tr>
<td width="35%"><b>hive.query.string</b></td><td width="65%">
INSERT OVERWRITE TABLE rankings_uservisits_join SELECT sourceIP, avg(pageRank), sum(adRevenue) as totalRevenue FROM rankings R JOIN (SELECT sourceIP, destURL, adRevenue FROM uservisits UV WHERE UV.visitDate &gt; '1999-01-01' AND UV.visitDate &lt; '2000-01-01') NUV ON (R.pageURL = NUV.destURL) GROUP BY sourceIP ORDER BY totalRevenue DESC LIMIT 1</td>
</tr>
<tr>
<td width="35%"><b>mapred.working.dir</b></td><td width="65%">hdfs://ec2-50-19-196-189.compute-1.amazonaws.com/user/ubuntu</td>
</tr>
<tr>
<td width="35%"><b>mapred.job.reduce.input.buffer.percent</b></td><td width="65%">0.0</td>
</tr>
</tbody>
</table>


<br>
<hr />
<a href="http://www.cloudera.com/hadoop/">Cloudera's Distribution including Apache Hadoop</a>, 2012.
</body></html>

